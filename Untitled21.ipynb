{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a278bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predicted 'spam' samples: 1290\n",
      "Number of predicted 'ham' samples: 1606\n",
      "Probabilities for class 'ham':\n",
      "Feature 'a': 0.19992573678921643\n",
      "Feature 'b': 0.05599915506005214\n",
      "Feature 'c': 0.07160365497395692\n",
      "Feature 'd': 0.09969470902289301\n",
      "Feature 'e': 0.2605703847173161\n",
      "Feature 'f': 0.05522651149625809\n",
      "Feature 'g': 0.08869783971548248\n",
      "Feature 'h': 0.1304426925462195\n",
      "Feature 'i': 0.18592784389929393\n",
      "Feature 'j': 0.020829213817627236\n",
      "Feature 'k': 0.06715069794239252\n",
      "Feature 'l': 0.13570036433931082\n",
      "Feature 'm': 0.09468500193357754\n",
      "Feature 'n': 0.17226397607867458\n",
      "Feature 'o': 0.22369708902725138\n",
      "Feature 'p': 0.059979615227022005\n",
      "Feature 'q': 0.004393307861373149\n",
      "Feature 'r': 0.13546159796943077\n",
      "Feature 's': 0.14846104996171602\n",
      "Feature 't': 0.21306836617140731\n",
      "Feature 'u': 0.10578560154834703\n",
      "Feature 'v': 0.03733751086876598\n",
      "Feature 'w': 0.07643235704023699\n",
      "Feature 'x': 0.01329327048732097\n",
      "Feature 'y': 0.09814985292064876\n",
      "Feature 'z': 0.008772253754163666\n",
      "Feature '0': 0.0019164258397885046\n",
      "Feature '1': 0.002693534829183116\n",
      "Feature '2': 0.00788101608605742\n",
      "Feature '3': 0.0023533432206991715\n",
      "Feature '4': 0.005754421575097256\n",
      "Feature '5': 0.0023071246016613763\n",
      "Feature '6': 0.0017358142665206374\n",
      "Feature '7': 0.0011054572979098123\n",
      "Feature '8': 0.0018686256864307307\n",
      "Feature '9': 0.0013958714458488532\n",
      "Feature ' ': 0.5825048525472913\n",
      "\n",
      "Probabilities for class 'spam':\n",
      "Feature 'a': 0.1777104324495451\n",
      "Feature 'b': 0.05988735271210033\n",
      "Feature 'c': 0.12064256084932963\n",
      "Feature 'd': 0.08726940219577384\n",
      "Feature 'e': 0.26493332695174865\n",
      "Feature 'f': 0.0634956147101732\n",
      "Feature 'g': 0.06037684953471445\n",
      "Feature 'h': 0.07308572204045982\n",
      "Feature 'i': 0.13882341342324858\n",
      "Feature 'j': 0.01647396871477154\n",
      "Feature 'k': 0.0430855077195291\n",
      "Feature 'l': 0.13536873753901416\n",
      "Feature 'm': 0.08711747449718983\n",
      "Feature 'n': 0.155250948249479\n",
      "Feature 'o': 0.22435064458470566\n",
      "Feature 'p': 0.09397355893437243\n",
      "Feature 'q': 0.012332240807650288\n",
      "Feature 'r': 0.17156596943733615\n",
      "Feature 's': 0.14978573445004034\n",
      "Feature 't': 0.20940252721666167\n",
      "Feature 'u': 0.09719663243789711\n",
      "Feature 'v': 0.04872043169383895\n",
      "Feature 'w': 0.08512932574206385\n",
      "Feature 'x': 0.060521331704760685\n",
      "Feature 'y': 0.07293505017097154\n",
      "Feature 'z': 0.01948048460995272\n",
      "Feature '0': 0.1933391330767202\n",
      "Feature '1': 0.10033433173860484\n",
      "Feature '2': 0.08359634209876575\n",
      "Feature '3': 0.054750673204537595\n",
      "Feature '4': 0.05754964619549518\n",
      "Feature '5': 0.06677592635366714\n",
      "Feature '6': 0.05716840017409731\n",
      "Feature '7': 0.06437376975767553\n",
      "Feature '8': 0.08178333558855848\n",
      "Feature '9': 0.04967566947944382\n",
      "Feature ' ': 0.5761910520226604\n",
      "\n",
      "Accuracy: 0.9136740331491713\n",
      "Precision: 0.9643410852713178\n",
      "Recall: 0.8591160220994475\n",
      "Confusion matrix:\n",
      " [[1402   46]\n",
      " [ 204 1244]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "import string\n",
    "import re\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Load SMS data\n",
    "sms_data = pd.read_csv(\"C:/Users/dell/AppData/Local/Programs/Python/Python311/SMSSpamCollection.csv\", sep=\"\\t\", header=None, names=[\"label\", \"text\"])\n",
    "\n",
    "# Preprocess the text column\n",
    "sms_data['text'] = sms_data['text'].fillna('')  # Replace missing values with empty string\n",
    "sms_data['text'] = sms_data['text'].str.lower()  # Convert all characters to lowercase\n",
    "sms_data['text'] = sms_data['text'].str.replace('[^\\w\\s]', '')  # Remove punctuation\n",
    "sms_data['text'] = sms_data['text'].str.strip()  # Remove leading/trailing white space\n",
    "\n",
    "# Save the preprocessed dataset\n",
    "sms_data.to_csv(\"preprocessed_dataset.csv\", index=False)\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "preprodata = pd.read_csv(\"preprocessed_dataset.csv\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "# Separate spam and ham samples\n",
    "spam_data = preprodata[preprodata[\"label\"] == \"spam\"]\n",
    "ham_data = preprodata[preprodata[\"label\"] == \"ham\"]\n",
    "\n",
    "# Upsample minority class\n",
    "spam_data_upsampled = resample(spam_data, \n",
    "                               replace=True,     # sample with replacement\n",
    "                               n_samples=len(ham_data),    # to match majority class\n",
    "                               random_state=123) # reproducible results\n",
    "\n",
    "ham_train, ham_test = train_test_split(ham_data, test_size=0.3)\n",
    "spam_train, spam_test = train_test_split(spam_data_upsampled, test_size=0.3)\n",
    "\n",
    "#print(\"Shape of ham_data:\", ham_data.shape)\n",
    "#print(\"Shape of spam_data:\", spam_data.shape)\n",
    "\n",
    "#print(\"Shape of ham_train:\", ham_train.shape)\n",
    "#print(\"Shape of spam_train:\", spam_train.shape)\n",
    "\n",
    "#print(\"Shape of ham_test:\", ham_test.shape)\n",
    "#print(\"Shape of spam_test:\", spam_test.shape)\n",
    "\n",
    "# Concatenate ham_train and spam_train\n",
    "train_data = pd.concat([ham_train, spam_train], axis=0)\n",
    "train_data = train_data.sample(frac=1, random_state=42)\n",
    "#print(train_data.iloc[:10, :10].to_string(index=False))\n",
    "\n",
    "# Concatenate ham_test and spam_test\n",
    "test_data = pd.concat([ham_test, spam_test], axis=0)\n",
    "test_data = test_data.sample(frac=1, random_state=42)\n",
    "\n",
    "# Define the vocabulary\n",
    "custom_vocab = list(string.ascii_lowercase + string.digits + ' ')\n",
    "#custom_vocab = list(set(\"\".join(train_data[\"text\"].tolist())))\n",
    "\n",
    "# Generate character n-gram features with custom vocabulary\n",
    "vectorizer = TfidfVectorizer(analyzer=\"char\", ngram_range=(1, 3), vocabulary=custom_vocab, lowercase=True)\n",
    "X_train = vectorizer.fit_transform(train_data['text'])\n",
    "\n",
    "# Fit and transform the training data to generate features\n",
    "#X_train = vectorizer.fit_transform(train_data['text'])\n",
    "\n",
    "#print(vectorizer.get_feature_names_out())\n",
    "#print(\"X_train\", X_train.toarray()[:10, :10])\n",
    "\n",
    "#Set print options to print entire array\n",
    "#np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "#Print X_train\n",
    "#print(train_data[['label', 'text']].head(500))    \n",
    "#print(X_train.toarray())\n",
    "\n",
    "# Fit and transform the training data to generate features\n",
    "X_train = vectorizer.fit_transform(train_data['text'])\n",
    "y_train = train_data['label']\n",
    "\n",
    "# Train a Multinomial Naive Bayes classifier with Laplace smoothing\n",
    "nb_classifier = MultinomialNB(alpha=1.0, fit_prior=True)\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Transform the test data to generate features\n",
    "X_test = vectorizer.transform(test_data['text'])\n",
    "y_test = test_data['label']\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "print(\"Number of predicted 'spam' samples:\", sum(y_pred == 'spam'))\n",
    "print(\"Number of predicted 'ham' samples:\", sum(y_pred == 'ham'))\n",
    "\n",
    "# Get the vocabulary\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Calculate the probabilities\n",
    "probabilities = {}\n",
    "#for i, label in enumerate(nb_classifier.classes_):\n",
    " #   feature_prob = np.exp(nb_classifier.feature_log_prob_[i])  # Convert log probabilities to linear scale\n",
    "  #  probabilities[label] = dict(zip(vocabulary, feature_prob))\n",
    "\n",
    "for i, label in enumerate(nb_classifier.classes_):\n",
    "    feature_prob = (nb_classifier.feature_count_[i] + 1) / (nb_classifier.class_count_[i] + X_train.shape[1])\n",
    "    probabilities[label] = dict(zip(vocabulary, feature_prob))    \n",
    "\n",
    "# Print the probabilities\n",
    "for label, prob in probabilities.items():\n",
    "    print(f\"Probabilities for class '{label}':\")\n",
    "    for feature, feature_prob in prob.items():\n",
    "        print(f\"Feature '{feature}': {feature_prob}\")\n",
    "    print()\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label=\"spam\")\n",
    "recall = recall_score(y_test, y_pred, pos_label=\"spam\")\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Confusion matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d392184b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
